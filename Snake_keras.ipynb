{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake AI\n",
    "\n",
    "A deep-learning agent learns how to play a snake from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Game\n",
    "\n",
    "Here we define the Game environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pixels=20\n",
    "\n",
    "class Game(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        npx_x=n_pixels # the board is an n_pixels x n_pixels grid \n",
    "        npx_y=n_pixels\n",
    "        \n",
    "        self.game_over=0 # flag indicating when the game is over\n",
    "        \n",
    "        self.score=0\n",
    "        \n",
    "        self.npx_x=npx_x\n",
    "        self.npx_y=npx_y\n",
    "        \n",
    "        self.pixels=np.zeros((npx_x,npx_y)) # each pixel takes either value 0 (empty), 1 (snake body), 2 (fruit), or 3 (snake head)\n",
    "        \n",
    "        self.snake=[] # the snake is a list of cells\n",
    "        self.snake.append(np.array([0,0]))\n",
    "        self.snake.append(np.array([0,1]))\n",
    "        self.len=2\n",
    "        \n",
    "        for i in range(self.len-1):\n",
    "            self.pixels[self.snake[i][0],self.snake[i][1]]=1\n",
    "        self.pixels[self.snake[-1][0],self.snake[-1][1]]=3\n",
    "        \n",
    "        self.dir=1 # 0 up, 1 right, 2 down, 3 left\n",
    "        \n",
    "        self.is_fruit=0 # flag indicating if there is a fruit on the board\n",
    "        self.fruit_pos=np.array([0,0])\n",
    "        self.spawn_fruit()\n",
    "        \n",
    "        self.count_from_fruit=0 # number of frames since the last time the snake ate\n",
    "        \n",
    "    def spawn_fruit(self,fruit_x=-1,fruit_y=-1):\n",
    "        # method spawning a fruit somewhere in the board. \n",
    "        # arguments:\n",
    "        # fruit_x (int): fruit position along the x direction. -1 corresponds to a random position.\n",
    "        # fruit_y (int): fruit position along the y direction. -1 corresponds to a random position.\n",
    "        \n",
    "        if self.is_fruit==1:\n",
    "            warnings.warn(\"Trying to spawn fruit, but there exist a fruit already\")\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if fruit_x>=0 and fruit_x < npx_x and fruit_y>=0 and fruit_y<npx_y:\n",
    "                self.fruit_pos=np.array([fruit_x,fruit_y])\n",
    "                self.pixels[self.fruit_pos[0],self.fruit_pos[1]]=2\n",
    "\n",
    "                self.is_fruit=1\n",
    "                \n",
    "            elif fruit_x==-1 and fruit_y==-1:\n",
    "        \n",
    "                count_black=self.npx_y-np.count_nonzero(self.pixels,axis=1)\n",
    "                \n",
    "                x_fruit=np.random.choice(range(self.npx_x),p=count_black/sum(count_black))\n",
    "                count_black=(self.pixels[x_fruit,:]==0).astype(int)\n",
    "                y_fruit=np.random.choice(range(self.npx_y),p=count_black/sum(count_black))\n",
    "\n",
    "                self.fruit_pos=np.array([x_fruit,y_fruit])\n",
    "                self.pixels[self.fruit_pos[0],self.fruit_pos[1]]=2\n",
    "\n",
    "                self.is_fruit=1\n",
    "                \n",
    "            else:\n",
    "                warnings.warn(\"Trying to spawn fruit outside of bounds\")\n",
    "        \n",
    "    def move_step(self):\n",
    "        # Moves the snake by 1 pixel along the direction self.dir\n",
    "        \n",
    "        head=self.snake[-1] # current position of the snake head\n",
    "        if self.dir==0:\n",
    "            newhead=head+np.array([-1,0])\n",
    "        elif self.dir==1:\n",
    "            newhead=head+np.array([0,1])\n",
    "        elif self.dir==2:\n",
    "            newhead=head+np.array([1,0])\n",
    "        elif self.dir==3:\n",
    "            newhead=head+np.array([0,-1])\n",
    "        # newhead: next position of the snake head    \n",
    "            \n",
    "        # if newhead is out of the board or is located on a body part, set game_over to 1 and give the agent a negative reward   \n",
    "        if newhead[0]<0 or newhead[0]>=self.npx_x or newhead[1]<0 or newhead[1]>=self.npx_y or self.pixels[newhead[0],newhead[1]]==1:\n",
    "            self.game_over=1\n",
    "            self.score+=self.count_from_fruit\n",
    "            \n",
    "            self.reward-=100\n",
    "            \n",
    "        # if newhead is located on the fruit, increase the size of the snake by updating the snake head   \n",
    "        elif self.pixels[newhead[0],newhead[1]]==2:\n",
    "            self.snake.append(newhead)\n",
    "            self.len+=1\n",
    "            self.is_fruit=0\n",
    "            self.score+=1100-self.count_from_fruit\n",
    "            self.count_from_fruit=0\n",
    "            self.pixels[newhead[0],newhead[1]]=3\n",
    "            self.pixels[self.snake[-2][0],self.snake[-2][1]]=1\n",
    "            \n",
    "            ## uncomment to avoid snake growth\n",
    "            #self.pixels[self.snake[0][0],self.snake[0][1]]=0 # delete this line to make snake grow\n",
    "            #del self.snake[0] # delete this line to make snake grow\n",
    "            \n",
    "            self.reward+=1000\n",
    "            \n",
    "        # otherwise remove the tail and add the new head   \n",
    "        else:\n",
    "            self.snake.append(newhead)\n",
    "            self.pixels[self.snake[-1][0],self.snake[-1][1]]=3\n",
    "            self.pixels[self.snake[-2][0],self.snake[-2][1]]=1\n",
    "            self.pixels[self.snake[0][0],self.snake[0][1]]=0\n",
    "            del self.snake[0]\n",
    "            \n",
    "    def set_direction(self,dir):\n",
    "        # Sets the new direction for the snake\n",
    "        # Arguments:\n",
    "        # dir: int: desired direction. Up = 0, right = 1, down = 2, left=3\n",
    "        \n",
    "        assert(dir>=-1)\n",
    "        assert(dir<=3)\n",
    "        \n",
    "        # forbidden_dir corresponds to the backward direction. If dir == forbidden_dir, the direction will not be updated\n",
    "        forbidden_dir=self.snake[-2]-self.snake[-1]\n",
    "        \n",
    "        if not (forbidden_dir-np.array([-1,0])).any():\n",
    "            if dir != 0 and dir!=-1:\n",
    "                self.dir=dir\n",
    "        elif not (forbidden_dir-np.array([0,1])).any():\n",
    "            if dir != 1 and dir!=-1:\n",
    "                self.dir=dir\n",
    "        elif not (forbidden_dir-np.array([1,0])).any():\n",
    "            if dir != 2 and dir!=-1:\n",
    "                self.dir=dir\n",
    "        elif not (forbidden_dir-np.array([0,-1])).any():\n",
    "            if dir != 3 and dir!=-1:\n",
    "                self.dir=dir\n",
    "        else:\n",
    "            warning.warn(\"Problem with forbidden_dir\")\n",
    "            \n",
    "    def time_step(self):\n",
    "        # defines the actions undertaken by the game during each frame\n",
    "        \n",
    "        # respawn the fruit\n",
    "        if self.is_fruit==0:\n",
    "            self.spawn_fruit()\n",
    "        \n",
    "        #move the snake\n",
    "        self.move_step()\n",
    "        self.count_from_fruit+=1\n",
    "        \n",
    "        # the snake starves to death if it hasn't eaten for 100 frames\n",
    "        if self.count_from_fruit>=100:\n",
    "            self.game_over=1\n",
    "            self.score+=self.count_from_fruit\n",
    "            \n",
    "            self.reward-=100\n",
    "        \n",
    "    def step(self,direction):\n",
    "        # defines the actions undertaken by the agent during each frame\n",
    "        # Arguments:\n",
    "        # direction: int: desired next direction. Up = 0, right = 1, down = 2, left=3\n",
    "        \n",
    "        self.reward=0\n",
    "        \n",
    "        # sets the new direction\n",
    "        self.set_direction(direction)\n",
    "            \n",
    "        # evolves the game for 1 frame    \n",
    "        self.time_step()\n",
    "        \n",
    "        info={'items':[]}\n",
    "        \n",
    "        return [self.get_input(), self.reward, self.game_over, info]\n",
    "    \n",
    "    def render(self,mode):\n",
    "        # renders the game \n",
    "        \n",
    "        pl.clf()\n",
    "        pl.imshow(self.pixels,vmin=0,vmax=3,cmap='gist_ncar')\n",
    "        pl.axis('off')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(pl.gcf())\n",
    "        \n",
    "    def reset(self):\n",
    "        # Resets the game to its initial state\n",
    "        \n",
    "        npx_x=n_pixels\n",
    "        npx_y=n_pixels\n",
    "        \n",
    "        self.game_over=0\n",
    "        \n",
    "        self.score=0\n",
    "        \n",
    "        self.npx_x=npx_x\n",
    "        self.npx_y=npx_y\n",
    "        \n",
    "        self.pixels=np.zeros((npx_x,npx_y)) # 0 nothing, 1 snake, 2 fruit, 3 head\n",
    "        \n",
    "        self.snake=[]\n",
    "        self.snake.append(np.array([0,0]))\n",
    "        self.snake.append(np.array([0,1]))\n",
    "        self.len=2\n",
    "        \n",
    "        for i in range(self.len-1):\n",
    "            self.pixels[self.snake[i][0],self.snake[i][1]]=1\n",
    "        self.pixels[self.snake[-1][0],self.snake[-1][1]]=3\n",
    "        \n",
    "        self.dir=1 # 0 up, 1 right, 2 down, 3 left\n",
    "        \n",
    "        self.is_fruit=0\n",
    "        self.fruit_pos=np.array([0,0])\n",
    "        self.spawn_fruit()\n",
    "        \n",
    "        self.count_from_fruit=0\n",
    "        \n",
    "        return self.get_input()\n",
    "   \n",
    "    def get_input(self):\n",
    "        # returns the information provided to the RL agent as a list of 24 int numbers. Along 8 directions, returns the distance from the snake head to the wall, its body, and the fruit\n",
    "        \n",
    "        input=[]\n",
    "        pixels=self.pixels[self.snake[-1][0]::-1,self.snake[-1][1]] # up\n",
    "        wall, body, fruit = self.search_wall_body_fruit(pixels)\n",
    "        input.append(wall)\n",
    "        input.append(body)\n",
    "        input.append(fruit)\n",
    "        \n",
    "        pixels=np.diagonal(np.flipud(self.pixels[:self.snake[-1][0]+1,self.snake[-1][1]:])) # up right\n",
    "        wall, body, fruit = self.search_wall_body_fruit(pixels)\n",
    "        input.append(wall)\n",
    "        input.append(body)\n",
    "        input.append(fruit)\n",
    "        \n",
    "        pixels=self.pixels[self.snake[-1][0],self.snake[-1][1]:] # right\n",
    "        wall, body, fruit = self.search_wall_body_fruit(pixels)\n",
    "        input.append(wall)\n",
    "        input.append(body)\n",
    "        input.append(fruit)\n",
    "        \n",
    "        pixels=np.diagonal(self.pixels[self.snake[-1][0]:,self.snake[-1][1]:]) # down right\n",
    "        wall, body, fruit = self.search_wall_body_fruit(pixels)\n",
    "        input.append(wall)\n",
    "        input.append(body)\n",
    "        input.append(fruit)\n",
    "        \n",
    "        pixels=self.pixels[self.snake[-1][0]:,self.snake[-1][1]] # down\n",
    "        wall, body, fruit = self.search_wall_body_fruit(pixels)\n",
    "        input.append(wall)\n",
    "        input.append(body)\n",
    "        input.append(fruit)\n",
    "        \n",
    "        pixels=np.diagonal(np.fliplr(self.pixels[self.snake[-1][0]:,:self.snake[-1][1]+1])) # down left\n",
    "        wall, body, fruit = self.search_wall_body_fruit(pixels)\n",
    "        input.append(wall)\n",
    "        input.append(body)\n",
    "        input.append(fruit)\n",
    "        \n",
    "        pixels=self.pixels[self.snake[-1][0],self.snake[-1][1]::-1] # left\n",
    "        wall, body, fruit = self.search_wall_body_fruit(pixels)\n",
    "        input.append(wall)\n",
    "        input.append(body)\n",
    "        input.append(fruit)\n",
    "        \n",
    "        pixels=np.diagonal(np.flipud(np.fliplr(self.pixels[:self.snake[-1][0]+1,:self.snake[-1][1]+1]))) # up left\n",
    "        wall, body, fruit = self.search_wall_body_fruit(pixels)\n",
    "        input.append(wall)\n",
    "        input.append(body)\n",
    "        input.append(fruit)\n",
    "        \n",
    "        return input\n",
    "\n",
    "    def search_wall_body_fruit(self,pixels):\n",
    "        # returns:\n",
    "        # wall: distance to the wall, given by the size of pixels\n",
    "        # body: distance to the snake body, corresponding to the first index with pixels[index]==1 (default value = n_pixels)\n",
    "        # fruit: distance to the snake body, corresponding to the first index with pixels[index]==2 (default value = 2*n_pixels)\n",
    "        \n",
    "        pixsize=pixels.size\n",
    "        \n",
    "        wall=pixsize-1\n",
    "        \n",
    "        body=self.npx_x\n",
    "        fruit= 2*self.npx_x\n",
    "        \n",
    "        count=-1\n",
    "        \n",
    "        while pixels.size>0:\n",
    "            if body==self.npx_x and pixels[0]==1:\n",
    "                body=count\n",
    "            elif fruit==2*self.npx_x and pixels[0]==2:\n",
    "                fruit=count\n",
    "            \n",
    "            count+=1\n",
    "            pixels=np.delete(pixels,0)\n",
    "            \n",
    "        return wall, body, fruit    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment given to the RL agent is then an instance of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep-learning architecture\n",
    "\n",
    "We use the keras library, and use a 3-layer neural network to approximate the Q-function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,24)))\n",
    "model.add(Dense(units=18))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=18))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is a DQNAgent, which uses the neural network model to chose the direction at each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = 4 # number of actions the agent can take, corresponding to the direction chosen at each time step\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000)\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=500,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent can then be trained (this may take a few hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn.fit(env, nb_steps=1000000, visualize=False,verbose=0)\n",
    "#dqn.save_weights(\"savefile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the agent in action! Feel free to change the seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA4lJREFUeJzt3DFuE1EUQNEZlAXgTUQsLz0SDAU928ObSBkqPhvIxbbw\n6GfIOfWX9Rpfv+LJ6xhjAXjNh9kDAG+XQABJIIAkEEASCCAJBJAEAkgCASSBANLD7AFes67fnHdy\nOI/P2+wRbvLz41gvvbFBAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAASSCA9CZPrW9xtPNWWJZlOZ+2\nXT733t8HGwSQBAJIAgEkgQCSQABJIIAkEEASCCAJBJAEAkiHP7Xe62QVsEEAfyEQQBIIIAkEkAQC\nSAIBJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJIIAkEEASCCAJBJAEAkgCASSBAJJAAEkg\ngCQQQBIIIAkEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJIIAkEEASCCAJBJAE\nAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJ\nIIAkEEASCCAJBJAEAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQ\nBAJIAgEkgQCSQABJIIAkEEASCCAJBJAEAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAA\nSSCAJBBAEgggCQSQBAJIAgEkgQCSQABJIIAkEEASCCAJBJAEAkgCASSBAJJAAEkggCQQQBIIIAkE\nkAQCSAIBJIEAkkAASSCAJBBAepg9AP9uPH2/+u364/OOk7xvj8/b7BHuzgYBJIEAkkAASSCAJBBA\nEgggCQSQBAJIAgEkl5T/AdeRx3M+bbNHWJZx+YkNAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIB\nJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJIIDkX61hgt8vX65+++nXXr/jXy++sEEASSCA\nJBBAEgggCQSQBAJIAgEkgQCSQABJIIDk1Bom2O98+r6OMSUwhUAASSCAJBBAEgggCQSQBAJIAgEk\ngQCSQADJqTXcyfm0zR7hNuPyExsEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQBAJI6xhX3FsC75IN\nAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJ\nIIAkEEASCCD9AeMRIqxvicx8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13bda6320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA4lJREFUeJzt3DFuE1EUQNEZlAXgTUQsLz0SDAU928ObSBkqPhvIxbbw\n6GfIOfWX9Rpfv+LJ6xhjAXjNh9kDAG+XQABJIIAkEEASCCAJBJAEAkgCASSBANLD7AFes67fnHdy\nOI/P2+wRbvLz41gvvbFBAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAASSCA9CZPrW9xtPNWWJZlOZ+2\nXT733t8HGwSQBAJIAgEkgQCSQABJIIAkEEASCCAJBJAEAkiHP7Xe62QVsEEAfyEQQBIIIAkEkAQC\nSAIBJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJIIAkEEASCCAJBJAEAkgCASSBAJJAAEkg\ngCQQQBIIIAkEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJIIAkEEASCCAJBJAE\nAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJ\nIIAkEEASCCAJBJAEAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQ\nBAJIAgEkgQCSQABJIIAkEEASCCAJBJAEAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAA\nSSCAJBBAEgggCQSQBAJIAgEkgQCSQABJIIAkEEASCCAJBJAEAkgCASSBAJJAAEkggCQQQBIIIAkE\nkAQCSAIBJIEAkkAASSCAJBBAepg9AP9uPH2/+u364/OOk7xvj8/b7BHuzgYBJIEAkkAASSCAJBBA\nEgggCQSQBAJIAgEkl5T/AdeRx3M+bbNHWJZx+YkNAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIB\nJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJIIDkX61hgt8vX65+++nXXr/jXy++sEEASSCA\nJBBAEgggCQSQBAJIAgEkgQCSQABJIIDk1Bom2O98+r6OMSUwhUAASSCAJBBAEgggCQSQBAJIAgEk\ngQCSQADJqTXcyfm0zR7hNuPyExsEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQBAJI6xhX3FsC75IN\nAkgCASSBAJJAAEkggCQQQBIIIAkEkAQCSAIBJIEAkkAASSCAJBBAEgggCQSQBAJIAgEkgQCSQABJ\nIIAkEEASCCD9AeMRIqxvicx8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13bda6320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "dqn.load_weights(\"keras_20x20\") # weights learned from playing on a 20 x 20 grid from scratch for a few hours\n",
    "\n",
    "obs=env.reset()\n",
    "dones=False\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "while not dones:\n",
    "    action = dqn.model.predict(np.reshape(obs,[1,1,24]))\n",
    "    obs, rewards, dones, info = env.step(np.argmax(action))\n",
    "    env.render(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
